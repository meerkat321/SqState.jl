export
    train,
    get_model

function get_model(model_name::String)
    f = jldopen(joinpath(model_path() , "$model_name.jld2"))
    model = f["model"]
    close(f)

    return model
end

function update_model!(model_file_path, model)
    model = cpu(model)
    jldsave(model_file_path; model)
    @warn "model updated!"
end

function train(model_name::String; epochs=10, Î·â‚€=1e-2, batch_size=25, n_validation_batch=100)
    if has_cuda()
        @info "CUDA is on"
        device = gpu
        CUDA.allowscalar(false)
    else
        device = cpu
    end

    m = model() |> device
    loss(x, y) = Flux.mse(m(x), y)
    # opt = Flux.Momentum(Î·â‚€, 0.9)
    opt = Flux.Optimiser(WeightDecay(1e-4), Flux.Momentum(Î·â‚€, 0.9))

    # prepare data
    data_file_names = filter(x->x!=".gitkeep", readdir(SqState.training_data_path()))
    loader_test = preprocess_q2args(data_file_names[1], batch_size=batch_size)
    @info "numbers of data fragments: $(length(data_file_names)-1)"
    data_loaders = Channel(5, spawn=true) do ch
        for e in 1:epochs, (i, file_name) in enumerate(data_file_names[2:end])
            put!(ch, preprocess_q2args(file_name, batch_size=batch_size))
            @info "Load epoch $e, file $i into buffer"
        end
    end

    t = 1
    losses = Float32[]
    data_validation = [(ğ±, ğ²) for (ğ±, ğ²) in loader_test][1:n_validation_batch] |> device
    function validate()
        validation_loss = sum(loss(ğ±, ğ²) for (ğ±, ğ²) in data_validation)/length(data_validation)
        @info "$(t)0k data\n Î·: $(opt.os[2].eta)\n loss: $validation_loss"

        push!(losses, validation_loss)
        (losses[end] == minimum(losses)) && update_model!(joinpath(model_path(), "$model_name.jld2"), m)
    end
    call_back = Flux.throttle(validate, 20, leading=false, trailing=true)

    for loader_train in data_loaders
        data = [(ğ±, ğ²) for (ğ±, ğ²) in loader_train] |> device
        @time Flux.train!(loss, params(m), data, opt, cb=call_back)
        (t % 30 == 0) && (opt.os[2].eta /= 2)
        t += 1
    end
end

function train_old_model(model_name::String; epochs=10, Î·â‚€=1e-2, batch_size=25, n_validation_batch=100)
    if has_cuda()
        @info "CUDA is on"
        device = gpu
        CUDA.allowscalar(false)
    else
        device = cpu
    end

    m = old_model() |> device
    loss(x, y) = Flux.mse(m(x), y)
    opt = Flux.ADAM(Î·â‚€)

    # prepare data
    data_file_names = filter(x->x!=".gitkeep", readdir(SqState.training_data_path()))
    loader_test = preprocess_q2args(data_file_names[1], batch_size=batch_size)
    @info "numbers of data fragments: $(length(data_file_names)-1)"
    data_loaders = Channel(5, spawn=true) do ch
        for e in 1:epochs, (i, file_name) in enumerate(data_file_names[2:end])
            put!(ch, preprocess_q2args(file_name, batch_size=batch_size))
            @info "Load epoch $e, file $i into buffer"
        end
    end

    t = 1
    losses = Float32[]
    data_validation = [(ğ±, ğ²) for (ğ±, ğ²) in loader_test][1:n_validation_batch] |> device
    function validate()
        validation_loss = sum(loss(ğ±, ğ²) for (ğ±, ğ²) in data_validation)/length(data_validation)
        @info "$(t)0k data\n Î·: $(opt.eta)\n loss: $validation_loss"

        push!(losses, validation_loss)
        (losses[end] == minimum(losses)) && update_model!(joinpath(model_path(), "$model_name.jld2"), m)
    end
    call_back = Flux.throttle(validate, 20, leading=false, trailing=true)

    for loader_train in data_loaders
        data = [(ğ±, ğ²) for (ğ±, ğ²) in loader_train] |> device
        @time Flux.train!(loss, params(m), data, opt, cb=call_back)
        (t > 30) && (opt.eta = 1e-2 / 2^ceil((t-30)/30))
        t += 1
    end
end
