export
    train,
    train_ae

function update_model!(model_path::String, model_name::String, model)
    model = cpu(model)
    jldsave(joinpath(model_path, "$model_name.jld2"); model)
    @warn "'$model_name' updated!"
end

function train(model_name::String; epochs=10, Î·â‚€=1e-2, batch_size=25)
    if has_cuda()
        @info "CUDA is on"
        device = gpu
        CUDA.allowscalar(false)
    else
        device = cpu
    end

    m = model() |> device
    loss(x, y) = Flux.mse(m(x), y)
    opt = Flux.Optimiser(WeightDecay(1e-4), Flux.Momentum(Î·â‚€, 0.9))

    # prepare data
    data_file_names = filter(x->x!=".gitkeep", readdir(SqState.training_data_path()))
    loader_test = preprocess_q2args(data_file_names[1], batch_size=batch_size)
    @info "numbers of data fragments: $(length(data_file_names)-1)"
    data_loaders = Channel(5, spawn=true) do ch
        for e in 1:epochs, (i, file_name) in enumerate(data_file_names[2:end])
            put!(ch, preprocess_q2args(file_name, batch_size=batch_size))
            @info "Load epoch $e, file $i into buffer"
        end
    end

    t = 1
    losses = Float32[]
    data_validation = [(ð±, ð²) for (ð±, ð²) in loader_test] |> device
    for loader_train in data_loaders
        @time begin
            data = [(ð±, ð²) for (ð±, ð²) in loader_train] |> device

            # training
            Flux.train!(loss, params(m), data, opt)

            # collect loss
            validation_loss = sum(loss(ð±, ð²) for (ð±, ð²) in data_validation)/length(data_validation)
            in_data_loss = sum(loss(ð±, ð²) for (ð±, ð²) in data)/length(data)
            @info "$(t)0k data\n Î·: $(opt.os[2].eta)\n in  loss: $in_data_loss\n out loss: $validation_loss"

            # update saved model
            push!(losses, validation_loss)
            (losses[end] == minimum(losses)) && update_model!(model_path(), model_name, m)

            # descent Î·
            (t > 50) && (opt.os[2].eta = Î·â‚€ / 2^ceil((t-50)/30))

            # update indicator
            t += 1
        end
    end
end

function train_ae(model_name::String; epochs=10, Î·â‚€=1e-4, batch_size=25)
    if has_cuda()
        @info "CUDA is on"
        device = gpu
        CUDA.allowscalar(false)
    else
        device = cpu
    end

    m = model_ae() |> device
    # loss(ð±, ð²) = sum(abs2, ð² .- m(ð±)) / size(ð±)[end]
    loss(x, y) = Flux.mse(m(x), y)
    opt = Flux.Optimiser(WeightDecay(1e-4), Flux.ADAM(Î·â‚€))

    # prepare data
    data_file_names = filter(x->x!=".gitkeep", readdir(SqState.training_data_path()))
    loader_test = preprocess_q2Ïƒs(data_file_names[1], batch_size=batch_size)
    @info "numbers of data fragments: $(length(data_file_names)-1)"
    data_loaders = Channel(5, spawn=true) do ch
        for e in 1:epochs, (i, file_name) in enumerate(data_file_names[2:end])
            put!(ch, preprocess_q2Ïƒs(file_name, batch_size=batch_size))
            @info "Load epoch $e, file $i into buffer"
        end
    end

    t = 1
    losses = Float32[]
    data_validation = [(ð±, ð²) for (ð±, ð²) in loader_test] |> device
    for loader_train in data_loaders
        @time begin
            data = [(ð±, ð²) for (ð±, ð²) in loader_train] |> device

            # training
            Flux.train!(loss, params(m), data, opt)

            # collect loss
            validation_loss = sum(loss(ð±, ð²) for (ð±, ð²) in data_validation)/length(data_validation)
            in_data_loss = sum(loss(ð±, ð²) for (ð±, ð²) in data)/length(data)
            @info "$(t)0k data\n Î·: $(opt.os[2].eta)\n in  loss: $in_data_loss\n out loss: $validation_loss"

            # update saved model
            push!(losses, validation_loss)
            (losses[end] == minimum(losses)) && update_model!(model_path(), model_name, m)

            # descent Î·
            (t > 50) && (opt.os[2].eta = Î·â‚€ / 2^ceil((t-50)/30))

            # update indicator
            t += 1
        end
    end
end
