########
# path #
########

training_data_path() = mkpath(joinpath(datadep"SqState", "training_data"))
model_path() = mkpath(joinpath(datadep"SqState", "model"))

############
# gen data #
############

rand2range(x_range) = x_range[1] + (x_range[2]-x_range[1])*rand()

function rand_arg_sqth_th(r_range, Î¸_range, nÌ„_range)
    r = rand2range(r_range)
    Î¸ = rand2range(Î¸_range)
    nÌ„ = rand2range(nÌ„_range)
    nÌ„0 = rand2range(nÌ„_range)
    c1 = rand()
    c2 = 1 - c1

    return r, Î¸, nÌ„, nÌ„0, c1, c2
end

function rand_arg_sqth(r_range, Î¸_range, nÌ„_range)
    r = rand2range(r_range)
    Î¸ = rand2range(Î¸_range)
    nÌ„ = rand2range(nÌ„_range)

    return r, Î¸, nÌ„
end

function construct_state_sqth_th(r, Î¸, nÌ„, nÌ„0, c1, c2, dim)
    state =
        c1 * SqueezedThermalState(Î¾(r, Î¸), nÌ„, dim=dim) +
        c2 * ThermalState(nÌ„0, dim=dim)

    return state
end

function construct_state_sqth(r, Î¸, nÌ„, dim)
    return SqueezedThermalState(Î¾(r, Î¸), nÌ„, dim=dim)
end

#########
# model #
#########

to_complex(ğ±1::AbstractArray, ğ±2::AbstractArray) = ğ±1 + im.*ğ±2

function ChainRulesCore.rrule(::typeof(to_complex), ğ±1::AbstractArray, ğ±2::AbstractArray)
    function to_complex_pullback(ğ²Ì„)
        return NoTangent(), real.(ğ²Ì„), imag.(ğ²Ì„)
    end

    return to_complex(ğ±1, ğ±2), to_complex_pullback
end

function reshape_cholesky(x)
    dim = Int(sqrt(size(x, 1)))
    ğ±_row = reshape(x, dim, dim, :)
    ğ±_real = cat([reshape(tril(ğ±_row[:, :, i]), dim, dim, 1) for i in axes(ğ±_row, 3)]..., dims=3)
    ğ±_imag = cat([reshape(tril(ğ±_row[:, :, i]', -1), dim, dim, 1) for i in axes(ğ±_row, 3)]..., dims=3)
    ğ± = to_complex(ğ±_real, ğ±_imag)

    return ğ±
end

function cholesky2Ï(x)
    ğ± = reshape_cholesky(Zygote.hook(real, x))
    ğ›’ = Flux.batched_mul(ğ±, Flux.batched_adjoint(ğ±))
    ğ›’ = cat([reshape(ğ›’[:, :, i]/tr(ğ›’[:, :, i]), 1, size(ğ›’, 1), size(ğ›’, 2), 1) for i in axes(ğ›’, 3)]..., dims=4)

    return vcat(real.(ğ›’), imag.(ğ›’))
end

function res_block(
    ch::NTuple{4, <:Integer},
    conv_kernel_size::NTuple{3, <:Integer},
    conv_pad::NTuple{3, <:Any},
    shortcut_kernel_size::Integer,
    shortcut_pad::Any,
    pool_size::Integer,
    Ïƒ=identity
)
    conv_layers = Chain(
        Conv((conv_kernel_size[1], ), ch[1]=>ch[2], Ïƒ,  pad=conv_pad[1]),
        # BatchNorm(ch[2], Ïƒ),
        Conv((conv_kernel_size[2], ), ch[2]=>ch[3], Ïƒ, pad=conv_pad[2]),
        # BatchNorm(ch[3], Ïƒ),
        Conv((conv_kernel_size[3], ), ch[3]=>ch[4], Ïƒ, pad=conv_pad[3]),
        # BatchNorm(ch[4]),
    )
    shortcut = Chain(
        Conv((shortcut_kernel_size, ), ch[1]=>ch[end], Ïƒ, pad=shortcut_pad),
        # BatchNorm(ch[end])
    )
    pool = (pool_size > 0) ? MeanPool((pool_size, )) : identity

    return Chain(
        Parallel(+, conv_layers, shortcut),
        x -> Ïƒ.(x),
        pool,
        BatchNorm(ch[end], Ïƒ)
    )
end

############
# training #
############

function update_model!(model_path::String, model_name::String, model)
    model = cpu(model)
    jldsave(joinpath(model_path, "$model_name.jld2"); model)
    @warn "'$model_name' updated!"
end

function get_device()
    if has_cuda()
        @info "CUDA is on"
        device = gpu
        CUDA.allowscalar(false)
    else
        device = cpu
    end

    return device
end

#############
# inference #
#############

function get_model(model_name::String)
    f = jldopen(joinpath(model_path() , "$model_name.jld2"))
    model = f["model"]
    close(f)

    return model
end
